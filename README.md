# Semi-Supervised Learning Theory

**Motivation**: understanding why semi-supervised learning (SSL) improves generalization performance. 
In particular, understanding why **pseudo-labeling** and **consistency regularization** aids learning. 

## Classical

#### Theory:
- [Unlabeled data: Now it helps, now it doesnâ€™t](https://papers.nips.cc/paper/2008/hash/07871915a8107172b3b5dc15a6574ad3-Abstract.html)
- [Pseudo-label: The simple and efficient semi-supervised learning method for deep neural
networks](https://github.com/emintham/Papers/blob/master/Lee-%20Pseudo-Label:%20The%20Simple%20and%20Efficient%20Semi-Supervised%20Learning%20Method%20for%20Deep%20Neural%20Networks.pdf)

- [Does unlabeled data provably help? worst-case analysis of the sample
complexity of semi-supervised learning](https://www.learningtheory.org/colt2008/papers/92-Ben-David.pdf)

## Modern

#### Methods:
- [Regularization With Stochastic Transformations and Perturbations for Deep Semi-Supervised Learning](https://arxiv.org/abs/1606.04586)
- [Mixup: Beyond Empirical Risk Minimization](https://arxiv.org/abs/1710.09412)
- [Mixmatch: MixMatch: A Holistic Approach to Semi-Supervised Learning](https://arxiv.org/abs/1905.02249)

- [Fixmatch: Simplifying semi-supervised learning with consistency and confidence](https://arxiv.org/abs/2001.07685)
- [Bootstrap your own latent: A new approach to self-supervised Learning](https://arxiv.org/pdf/2006.07733.pdf)
- [In Defense of Pseudo-Labeling: An Uncertainty-Aware Pseudo-label Selection Framework for Semi-Supervised Learning](https://arxiv.org/abs/2101.06329)
- [Meta Pseudo Labels](https://arxiv.org/abs/2003.10580)
- [Unsupervised Data Augmentation for Consistency Training](https://arxiv.org/pdf/1904.12848.pdf)
- [When Does Self-Supervision Improve Few-Shot Learning](https://arxiv.org/abs/1910.03560)
- [Rethinking Pre-training and Self-training](https://arxiv.org/abs/2006.06882)
- [Supervised contrastive learning](https://arxiv.org/abs/2004.11362)
- [BREEDS: Benchmarks for Subpopulation Shift](https://arxiv.org/abs/2008.04859)
- [Deep Self-Learning from Noisy Labels](https://openaccess.thecvf.com/content_ICCV_2019/html/Han_Deep_Self-Learning_From_Noisy_Labels_ICCV_2019_paper)
- [Does Label Smoothing Mitigate Label Noise?](https://arxiv.org/pdf/2003.02819)

### Theory:
- [Understanding Self-Training for Gradual Domain Adaptation](https://arxiv.org/pdf/2002.11361.pdf)
- [Predicting what you already know helps: Provable self-supervised learning](https://arxiv.org/pdf/2008.01064.pdf)
- [Self-Supervised Learning from a Multi-View Perspective](https://arxiv.org/pdf/2006.05576.pdf)


- [How Does Mixup Help Robustness and Generalization?](https://arxiv.org/pdf/2010.04819.pdf)
- [Safe Deep Semi-Supervised Learning for Unseen-Class Unlabeled Data](http://proceedings.mlr.press/v119/guo20i.html)
- [A Statistical Theory of Semi-Supervised Learning](https://arxiv.org/abs/2008.05913)

- [Unsupervised Label Noise Modeling and Loss Correction](https://arxiv.org/abs/1904.11238)
- [Theoretical Analysis of Self-Training with Deep Networks on Unlabeled Data]()
- [Self-training Avoids Using Spurious Features Under Domain Shift](https://arxiv.org/abs/2006.10032) 
- [Intriguing Properties of Contrastive Losses]()
- [Does learning require memorization? a short tale about a long tail]()
- [What Neural Networks Memorize and Why: Discovering the Long Tail via Influence Estimation]()

<!---
Q: Distinction between self-training, pseudo-labeling, and consistency regularization?
-->
